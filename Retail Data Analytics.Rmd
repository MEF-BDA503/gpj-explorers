---
title: "Retail Data Analytics"
author: "Bengisu Öniz, Mustafa Tilkat, Gökhan ??ahin, Ahmet Tunçel"
date: "18 Kasym 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```


#Our Objective
The main purpose of this project is to predict the department-wide sales for each store for the following year. Moreover, anaylzing the effects of markdowns on holiday and finding meaningful insights are objectives of this Project.

#About the Data

We have found a data set from Kaggle. The data set named [Retail Data Analytics](https://www.kaggle.com/manjeetsingh/retaildataset) which is about one of the retail company’s sales.

There are historical sales data for 45 stores located in different regions - each store contains a number of departments. The company also runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the.Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks.
Within the Excel Sheet, there are 3 Tabs – Stores, Features and Sales.

**Stores**
Anonymized information about the 45 stores, indicating the type and size of store.

**Features**
Contains additional data related to the store, department, and regional activity for the given dates.
+Store - the store number
+Date - the week
+Temperature - average temperature in the region
+Fuel_Price - cost of fuel in the region
+MarkDown1-5 - anonymized data related to promotional markdowns. Mark Down data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA
+CPI - the consumer price index
Unemployment - the unemployment rate
+Is Holiday -whether the week is a special holiday week

**Sales**
Historical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab there are the following fields:
+Store - the store number
+Dept - the department number
+Date - the week
+Weekly_Sales - sales for the given department in the given store
+IsHoliday - whether the week is a special holiday week’[1]  


#Exploratory Analysis

## Loading Libraries

```{r}
library(lubridate)
library(tidyverse)
library(data.table)
library(stringr)
library(ggplot2)
library(plotly)
library(corrplot)
library(xgboost)
library(DiagrammeR)
```
We downloaded data set from 3 csv files. 

##Data Loading

```{r warning=FALSE}
  features_data_set <- read.csv2("Features data set.csv", header = TRUE, sep = ",")
  sales_data_set <- read.csv2("sales data-set.csv", header = TRUE, sep = ",")
  stores_data_set <- read.csv2("stores data-set.csv", header = TRUE, sep = ",")

```

```{r}
  str(features_data_set)
  
```
There are 8190 observations and 12 variables at Features data set. These data set gives the internal and external conditions of the situations.

```{r}
str(sales_data_set)
```
There are 421570 observations and 5 variables at Sales data set. These data set gives information detailed information about sales.

```{r}
str(stores_data_set)
```
There are 45 observations which are store numbers. The data shows the types and sizes of the stores. We merged the 3 data sets via store numbers.

### Data type converting
We needed to see months and years at our data set. Therefore, we splitted dates by using the places of the character. 
```{r}

features_data_set$Year <- substr(features_data_set$Date, 7, 10)
features_data_set$Month <- substr(features_data_set$Date, 4, 5)
features_data_set$Day <- substr(features_data_set$Date, 1, 2)

sales_data_set$Year <- substr(sales_data_set$Date, 7, 10)
sales_data_set$Month <- substr(sales_data_set$Date, 4, 5)
sales_data_set$Day <- substr(sales_data_set$Date, 1, 2)

sales_data_set$Weekly_Sales <- as.character(sales_data_set$Weekly_Sales)
sales_data_set$Weekly_Sales <- as.numeric(sales_data_set$Weekly_Sales,2)

```

### Looking at the store numbers

```{r}
 ggplot(stores_data_set, aes(Type, fill = Type ) ) +
  geom_bar() +
  xlab("Type of Store") + ylab("Count of Store")

```

We had a look at count of the different types of the stores. There are more stores with Type A.

### Looking at the Sales of the Years
```{r}

YearSales <- sales_data_set %>% group_by(Year,Dept) %>% summarise(YearSales = sum(Weekly_Sales)) %>% arrange(desc(YearSales))



ggplot(head(YearSales, 60), aes(Year, YearSales)) +
  geom_col() + facet_wrap(~Dept)

```

We looked the yearly sales of the all stores. The highest yearly sales happened in 2011.  

### Analyzing the store sizes

```{r}
SalesStore <- left_join(sales_data_set, stores_data_set, by = "Store")

ggplot(SalesStore, aes(Type, Size) ,log = "xy") +
  geom_point()
```

We wanted to know if there is a relationship between store types and sizes. As you can see the most of the stores with type A have larger sizes.  


###Looking at the relationship between Store Sizes & Weekly Sales

```{r}
plot(SalesStore$Size,SalesStore$Weekly_Sales, main = "Size vs Sales", xlab = "Store Size", ylab = "Weekly Sales")

```


```{r}

SalesStore <- left_join(sales_data_set, stores_data_set, by = "Store")
monthsales<-SalesStore %>% group_by(Month) %>% summarise(montlysales=sum(Weekly_Sales))
monthsales$montlysales <- as.numeric(monthsales$montlysales)


qplot(x =Month , y = montlysales,data = monthsales)


```

Then it was time to see the montly sales trend of the stores. The highest sales occurred on April and July. 


```{r}
deptSalesdata <- sales_data_set %>% group_by(Dept) %>% summarise(deptSales = sum(Weekly_Sales)) %>% arrange(desc(deptSales))
deptSalesdata$Dept<-as.factor(deptSalesdata$Dept)

deptSalesdata<-data.frame(deptSalesdata)




ggplot(deptSalesdata,aes(x=Dept,y=deptSales,fill=Dept)) +geom_bar(fill="#56B4E6", stat = "identity") + scale_x_discrete(name="Departments") + theme( axis.text.x = element_text(angle =90)) + ggtitle('Sales of the Departments')
```

There were 99 departments. We wanted to see the sales trend of the departments. The departments that have the highest sales are.......


```{r}
features_data_set$Temperature<-as.numeric(as.vector(features_data_set$Temperature))
features_data_set$Unemployment<-as.numeric(as.vector(features_data_set$Unemployment))
features_data_set$Fuel_Price<-as.numeric(as.vector(features_data_set$Fuel_Price))

sales_data_set$Weekly_Sales<-as.numeric(as.vector(sales_data_set$Weekly_Sales))

features_temp_m <- features_data_set %>% group_by(Month) %>% summarise(ort_temp=mean(Temperature))


sales_m <- sales_data_set %>%group_by(Month) %>% summarise(ort_sa=mean(Weekly_Sales))

temp_sales <- inner_join(sales_m,features_temp_m,by="Month")

ggplot(temp_sales, aes(x = Month, y = ort_temp, size = ort_sa)) +
  geom_point(shape = 21,colour = "#000000", fill = "#40b8d0")



```
We wanted to see how the external conditions affected sales. We used the temperature data from features. The highest sales accured when the temperature was lowest and highest.

```{r}

features_Unem_m <- features_data_set %>% group_by(Month) %>% summarise(avg_une=mean(Unemployment))

Unem_sales <- inner_join(sales_m,features_Unem_m,by="Month")




ggplot(Unem_sales, aes(x = Month, y = avg_une, size = ort_sa)) +
  geom_point(shape = 21,colour = "#000000", fill = "#40b8d0")
```





```{r}
SalesStorepca <- left_join(sales_data_set, stores_data_set, by = "Store")

alldatapca <- inner_join(SalesStorepca, features_data_set , by = c("Store", "Year", "Month", "Day")) 

selectcolpca <- alldatapca %>% select( Size, CPI ,Unemployment ,Fuel_Price, Weekly_Sales, Temperature)

selectcolpca$CPI <- as.numeric(as.character(selectcolpca$CPI))
selectcolpca$Unemployment <- as.numeric(as.character(selectcolpca$Unemployment))
selectcolpca$Fuel_Price <- as.numeric(as.character(selectcolpca$Fuel_Price))
selectcolpca$Weekly_Sales <- as.numeric(as.character(selectcolpca$Weekly_Sales))
selectcolpca$Temperature <- as.numeric(as.character(selectcolpca$Temperature))

prout <- prcomp(selectcolpca, center = TRUE ,scale = TRUE)

prvar <- prout$sdev^2

pve <- prvar / sum(prvar)

plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")


# Plot cumulative proportion of variance explained

plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

```


Variability of each principal component and variance explained by each principal component.


```{r}
alldata <- inner_join(SalesStore, features_data_set , by = c("Store", "Year", "Month", "Day")) 

selectcol <- alldata %>% select( Size, CPI ,Unemployment ,Fuel_Price, Weekly_Sales, Temperature)


selectcol$CPI <- as.numeric(as.character(selectcol$CPI))
selectcol$Unemployment <- as.numeric(as.character(selectcol$Unemployment))
selectcol$Fuel_Price <- as.numeric(as.character(selectcol$Fuel_Price))
selectcol$Weekly_Sales <- as.numeric(as.character(selectcol$Weekly_Sales))
selectcol$Temperature <- as.numeric(as.character(selectcol$Temperature))

matrixdata <- as.matrix(selectcol)

corrplot(cor(selectcol) ,method = "circle")

```




Looking at the percentages of the montly sales by the stores.

```{r}
Sales<-data.table(sales_data_set)
Features<-data.table(features_data_set)
Stores<-data.table(stores_data_set)

Sales<-Sales[,list(Store,Dept,Date,Weekly_Sales)]

setkey(Sales,Store,Date)
setkey(Features,Store,Date)

Sales<-Features[Sales]

setkey(Sales,Store)
setkey(Stores,Store)

Sales<-Stores[Sales]

#str(Sales)
#summary(Sales)

head(Sales)
```
we also prepared the data with data.table package that is hepful for manupulating big data faster than dplyr.




```{r}
daysales<-Sales %>%
 group_by(Day) %>%
 summarise(Salesofthedays=sum(Weekly_Sales))

daysales$Day<-as.factor(daysales$Day)


ggplot(daysales,aes(x=Day,y=Salesofthedays,fill=Day)) +geom_bar(fill="#FF6666", stat = "identity") + scale_x_discrete(name="Days") + theme( axis.text.x = element_text(angle =90)) + ggtitle('Sales of the Days')

```

In addition to the other sale trend graphics, this graphics shows the Daily sales in the month. The sales decrease on the weekends.



```{r}
YearlySales<-Sales[,sum(Weekly_Sales,na.rm = TRUE),.(Store,Type,Size)]

setnames(YearlySales,"V1","Yearly_Sales")

ggplot(YearlySales,aes(x=Size,y=Yearly_Sales)) +
 geom_point()+
 geom_smooth(method=lm,color="RED",se = FALSE)+
 scale_x_continuous(waiver()) + scale_y_continuous(waiver())
```

We expected to see there is a strong relationship between the sales and the store sizes. We checked it on this graph. As you can see relationship is positive and strong.



```{r}
Sales[,Month:=as.numeric(substring(as.character(Date),4,5))]
Sales[,Year:=as.numeric(substring(as.character(Date),7,10))]
Sales$Date<-dmy(Sales$Date)

Sales[,Week:=week(Date)]
Sales[,YearWeek:=as.numeric(ifelse(Week<10,paste(Year,"0",Week,sep=""),paste(Year,Week,sep="")))]
Sales[,SizeC:=ifelse(Type=="A",1,ifelse(Type=="B",2,3))]

```

We created year, month, week, yearweek coloumns by using data.table package.

```{r}

MonthlySales<-Sales[,sum(Weekly_Sales,na.rm = TRUE),.(Store,Month)]

setnames(MonthlySales,"V1","Monthly_Sales")

MonthlySales[,TotalSales:=sum(Monthly_Sales,na.rm = TRUE),.(Month)]

MonthlySales[,SalesPercantage:=Monthly_Sales*1.0/TotalSales]

summary(MonthlySales[,SalesPercantage])

```

We created MonthlySales table to see the montly sales of three years, to calculate the percantages of the store in total monthly sales.






###Clustering of the stores according to the montly sales with k-means

We wanted to cluster the data to group the stores that have the same patterns.

Firstly, we looked at the percantage of the stores on the total sales at the related month.
Then visualized the clustering.
```{r}

Clusno<-5  #number of clusters was set.

#store/month matrix had created with dcast function.

CM=dcast.data.table(MonthlySales,Store~Month,value.var="SalesPercantage")

S<-colnames(CM)
CM<-data.frame(CM)
CM[is.na(CM)]=0    #NA's was filled with 0.
colnames(CM)<-S
CM<-data.table(CM)

head(CM)

# basl<-which(colnames(rr)=="2")
# bitis<-which(colnames(rr)=="269")
set.seed(7)
CM[,clusno:=kmeans(CM[,c(2:ncol(CM)),with=F],Clusno)$cluster]  

clusters<-CM[,list(Store,clusno)]  #clusters created with k-mean.

setkey(clusters,Store)

setkey(MonthlySales,Store) 

MonthlySales<-clusters[MonthlySales] #MonthlySales and Clusters tables were merged.

SalesP<-dcast.data.table(MonthlySales,Month~Store,value.var="SalesPercantage") #store-month matrix had created and filled with sales percantages

MonthlySales$Month <- factor(MonthlySales$Month)
MonthlySales$Store <- factor(MonthlySales$Store)
MonthlySales$clusno <- factor(MonthlySales$clusno)

#month, store, clusno variables were converted as factor.

```

```{r}
# plotting reference lines across each facet:

referenceLines <- MonthlySales  # \/ Rename
colnames(referenceLines)[2] <- "groupVar"
zp <- ggplot(MonthlySales,
             aes(x = Month, y = SalesPercantage))
zp <- zp + geom_line(data = referenceLines,  # Plotting the "underlayer"
                     aes(x = Month, y = SalesPercantage, group = groupVar),
                     colour = "BLUE", alpha = 1/2, size = 1/2)
zp <- zp + geom_line(size = 1)  # Drawing the "overlayer"
zp <- zp + facet_wrap(~ Store)
zp <- zp + theme_bw()
ggplotly()

ggplot(MonthlySales, aes(x=Month, y=SalesPercantage, color=clusno, group=Store)) +
  geom_line()

ggplotly()

```

In this graph every line refers to a store and line shows the monthly sales trend. Everline with the same colour refers to a cluster group.

```{r}

Features <- read.csv("Features data set.csv", header = TRUE, sep = ",")
Sales <- read.csv("sales data-set.csv", header = TRUE, sep = ",")
Stores <- read.csv("stores data-set.csv", header = TRUE, sep = ",")

Sales<-data.table(Sales)
Features<-data.table(Features)
Stores<-data.table(Stores)

Sales<-Sales[,list(Store,Dept,Date,Weekly_Sales)]

setkey(Sales,Store,Date)
setkey(Features,Store,Date)

Sales<-Features[Sales]

setkey(Sales,Store)
setkey(Stores,Store)

Sales<-Stores[Sales]

str(Sales)
summary(Sales)

Sales[,Month:=as.numeric(substring(as.character(Date),4,5))]
Sales[,Year:=as.numeric(substring(as.character(Date),7,10))]
Sales$Date<-dmy(Sales$Date)

Sales[,Week:=week(Date)]
Sales[,YearWeek:=as.numeric(ifelse(Week<10,paste(Year,"0",Week,sep=""),paste(Year,Week,sep="")))]
Sales[,SizeC:=ifelse(Type=="A",1,ifelse(Type=="B",2,3))]

SalesWeeklybyStores<-Sales[,sum(Weekly_Sales),.(Year,Month,Week,YearWeek,Store,Type,Size,Temperature,Fuel_Price,CPI,Unemployment,IsHoliday)]

SalesWeeklybyStores[,IsHoliday:=ifelse(IsHoliday=="FALSE",0,1)]

setnames(SalesWeeklybyStores,"V1","WeeklySales")

str(SalesWeeklybyStores)

setkey(SalesWeeklybyStores,Store,YearWeek)
SalesWeeklybyStores[, Weekly_Sales_lag1:=dplyr::lag(WeeklySales,1),.(Store)]
SalesWeeklybyStores[, Fuel_Price_lag1:=dplyr::lag(Fuel_Price,1),.(Store)]
SalesWeeklybyStores[, Temperature_lag1:=dplyr::lag(Temperature,1),.(Store)]


trainset = SalesWeeklybyStores[YearWeek<=201215 &YearWeek>201014,]
testset = SalesWeeklybyStores[YearWeek>201215,]
testset[, id := .I]

summary(trainset) 

RMPSE<- function(preds, dtrain) {
  labels <- getinfo(dtrain, "label")
  elab<-exp(as.numeric(labels))-1
  epreds<-exp(as.numeric(preds))-1
  err <- sqrt(mean((epreds/elab-1)^2))
  return(list(metric = "RMPSE", value = err))
}

nrow(SalesWeeklybyStores)

h<-sample(nrow(trainset),600)

dval<-xgb.DMatrix(data=data.matrix(trainset[h,]),label=log(trainset$WeeklySales+1)[h])
dtrain<-xgb.DMatrix(data=data.matrix(trainset[-h,]),label=log(trainset$WeeklySales+1)[-h])
watchlist<-list(val=dval,trainset=dtrain)
param <- list(  objective           = "reg:linear",
                booster = "gbtree",
                eta                 = 0.25, # 0.06, #0.01,
                max_depth           = 5, #changed from default of 8
                subsample           = 0.7, # 0.7
                colsample_bytree    = 0.7 # 0.7
                
                # alpha = 0.0001,
                # lambda = 1
)

selectseed<-function(a,b){

bestseed2 = data.table(s = 201, x = 1.1, y = 1.1)

for (s in a:b){
  
  set.seed(s)
  set.seed(s)
  

clf <- xgb.train(   params              = param,
                    data                = dtrain,
                    nrounds             = 30, #100, #280, #125, #250, # changed from 300
                    verbose             = 0,
                    early.stop.round    = 25,
                    watchlist           = watchlist,
                    maximize            = FALSE,
                    feval=RMPSE
)
testset<-data.frame(testset)

feature.names <- names(testset)[-c(13,17)]


pred1 <- exp(predict(clf, data.matrix(testset[,feature.names]))) -1

pred1<-data.table(pred1)

submission <- data.frame(id=testset$id, Sales=pred1)

submission<-data.table(submission)
testset<-data.table(testset)

setkey(submission,id)
setkey(testset,id)

final<-testset[submission]

final[,Err:=abs(pred1-WeeklySales)/WeeklySales]
final2<-final[,list(sum(WeeklySales),sum(abs(pred1-WeeklySales)),sum(pred1)),.(Store)]

setnames(final2,"V1","TWS")
setnames(final2,"V2","TE")
setnames(final2,"V3","TP")

x<-final2[,sum(TE)/sum(TWS)]
y<-final[,mean(Err)]

bestseed<-data.table(s,x,y)
bestseed2<-rbind(bestseed,bestseed2)

}

seed<-head(bestseed2[order(x)],1)[]$s
return(seed)
}

set.seed(selectseed(1,60))
set.seed(selectseed(1,60))

clf <- xgb.train(   params              = param,
                    data                = dtrain,
                    nrounds             = 30, #100, #280, #125, #250, # changed from 300
                    verbose             = 1,
                    early.stop.round    = 25,
                    watchlist           = watchlist,
                    maximize            = FALSE,
                    feval=RMPSE
)
testset<-data.frame(testset)

feature.names <- names(testset)[-c(13,17)]


pred1 <- exp(predict(clf, data.matrix(testset[,feature.names]))) -1

pred1<-data.table(pred1)

submission <- data.frame(id=testset$id, Sales=pred1)

submission<-data.table(submission)
testset<-data.table(testset)

setkey(submission,id)
setkey(testset,id)

final<-testset[submission]

final[,Err:=abs(pred1-WeeklySales)/WeeklySales]
final2<-final[,list(sum(WeeklySales),sum(abs(pred1-WeeklySales)),sum(pred1)),.(Store)]

setnames(final2,"V1","TWS")
setnames(final2,"V2","TE")
setnames(final2,"V3","TP")

```

```{r}

final2[,sum(TE)/sum(TWS)]
final[,mean(Err)]

```

```{r}
model <- xgb.dump(clf, with.stats = T)
model[1:10] #This statement prints top 10 nodes of the model

names <- dimnames(data.matrix(testset[,-c(13,17)]))[[2]]

importance_matrix <- xgb.importance(names, model = clf)

xgb.plot.importance(importance_matrix[1:5,])
```


```{r}
xgb.plot.tree(model = clf, n_first_tree = 10) #ilk 10 aðaç
```

```{r}
ggplot(final, aes(YearWeek)) + 
  geom_line(aes(y = WeeklySales, colour = "WeeklySales")) + 
  geom_line(aes(y = pred1, colour = "pred1")) +
  facet_wrap(~final$Store) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggplotly()
```

  
  
  ###References
  #_Retail Data Analytics. (2017, August). Retrieved from https://www.kaggle.com/manjeetsingh/retaildataset_
  
